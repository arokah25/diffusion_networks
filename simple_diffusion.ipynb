{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm # a nice progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dataset of 1D data from a mixture of two Gaussians\n",
    "# this is a simple example, but you can use any distribution\n",
    "data_distribution = torch.distributions.mixture_same_family.MixtureSameFamily(\n",
    "    torch.distributions.Categorical(torch.tensor([1, 2])),  #ratio 1:2 of falling into one distribution vs the other\n",
    "    torch.distributions.Normal(torch.tensor([-4., 4.]), torch.tensor([1., 1.]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_distribution.sample(torch.Size([10000]))  # create training data set\n",
    "dataset_validation = data_distribution.sample(torch.Size([1000])) # create validation data set\n",
    "\n",
    "# Plot the true distribution\n",
    "sns.histplot(dataset.numpy(), stat='density', bins=80, label='True Data')\n",
    "plt.legend()\n",
    "plt.title(\"True Data Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperparameters\n",
    "TIME_STEPS = 250\n",
    "BETA = torch.tensor(0.02)\n",
    "ALPHA = 1.0 - BETA\n",
    "ALPHABAR = torch.cumprod(torch.full((TIME_STEPS,), ALPHA), dim=0)\n",
    "\n",
    "N_EPOCHS = 1000\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 8e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisePredictorMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    inputs: a noisy sample (xt), time step (t)\n",
    "    outputs: a single scalar per sample -- the predicted standard normal noise\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Normalize time and concatenate with x_t\n",
    "        t = t.float().unsqueeze(1) / TIME_STEPS\n",
    "        x = x.unsqueeze(1)\n",
    "        return self.model(torch.cat([x, t], dim=1)).squeeze(1)\n",
    "\n",
    "g = NoisePredictorMLP()\n",
    "optimizer = torch.optim.Adam(g.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "epochs = tqdm(range(N_EPOCHS))\n",
    "for epoch in epochs:\n",
    "    g.train()\n",
    "    indices = torch.randperm(len(dataset))\n",
    "    for i in range(0, len(dataset) - BATCH_SIZE, BATCH_SIZE): #skip final incomplete batch\n",
    "        x0 = dataset[indices[i:i + BATCH_SIZE]]\n",
    "\n",
    "        # Sample time step and noise\n",
    "        t = torch.randint(0, TIME_STEPS, (BATCH_SIZE,)) #generate random time steps in the correct shape (BATCH_SIZE,)\n",
    "        alpha_bar = ALPHABAR[t].unsqueeze(1) #generate alphabar corresponding to each time step\n",
    "        epsilon = torch.randn_like(x0)\n",
    "\n",
    "        # Create noisy version x_t\n",
    "        xt = torch.sqrt(alpha_bar) * x0.unsqueeze(1) + torch.sqrt(1 - alpha_bar) * epsilon.unsqueeze(1)\n",
    "        xt = xt.squeeze(1)\n",
    "\n",
    "        # Predict epsilon and compute loss\n",
    "        pred_epsilon = g(xt, t)\n",
    "        loss = loss_fn(pred_epsilon, epsilon)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_reverse(g, count):\n",
    "    \"\"\"\n",
    "    Sample from the model by applying the reverse diffusion process\n",
    "\n",
    "    ----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    g : torch.nn.Module\n",
    "        The neural network that predicts the noise added to the data\n",
    "    count : int\n",
    "        The number of samples to generate in parallel\n",
    "\n",
    "    ----------\n",
    "    Returns\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        The final sample from the model\n",
    "    \"\"\"\n",
    "    g.eval()\n",
    "    x = torch.randn(count)  # Start from pure noise\n",
    "\n",
    "    for t in reversed(range(1, TIME_STEPS)):\n",
    "        t_batch = torch.full((count,), t, dtype=torch.long) #feed the current timestep t into the network for each sample in the batch\n",
    "        beta = BETA\n",
    "        alpha = ALPHA\n",
    "        alpha_bar = ALPHABAR[t]\n",
    "\n",
    "        z = torch.randn_like(x) if t > 1 else torch.zeros_like(x) #generate z to add stochasticity to the reverse step\n",
    "        pred_epsilon = g(x, t_batch) #predict the standard normal noise that was added to get xt using trained NN\n",
    "\n",
    "        x = (1 / torch.sqrt(alpha)) * (\n",
    "            x - (1 - alpha) / torch.sqrt(1 - alpha_bar) * pred_epsilon\n",
    "        ) + torch.sqrt(beta) * z\n",
    "\n",
    "    return x #final output x0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "#Plot generated samples against the underlying function used to generate training data\n",
    "# ---------------------------------------------\n",
    "samples = sample_reverse(g, 1000).numpy()\n",
    "\n",
    "sns.histplot(samples, stat='density', bins=50, color='red', label='Model Samples')\n",
    "sns.kdeplot(dataset.numpy(), color='blue', label='True Data', linewidth=2)\n",
    "plt.title(\"Comparison: Generated vs True Distribution\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Visualization: How samples are transformed with increasing diffusion steps\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Sample from the actual data distribution (mixture of Gaussians)\n",
    "x0_vis = data_distribution.sample(torch.Size([200]))  # same as training distribution\n",
    "\n",
    "# Select timesteps to visualize\n",
    "timesteps = [0, 50, 100, 150, 200, 249]\n",
    "xt_distributions = []\n",
    "\n",
    "# Apply forward diffusion at selected timesteps\n",
    "for t in timesteps:\n",
    "    alpha_bar_t = ALPHABAR[t]\n",
    "    eps = torch.randn_like(x0_vis)\n",
    "    xt = torch.sqrt(alpha_bar_t) * x0_vis + torch.sqrt(1 - alpha_bar_t) * eps\n",
    "    xt_distributions.append(xt)\n",
    "\n",
    "# Plot the evolution of the distribution over time\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8), sharey=True)\n",
    "axes = axes.flatten()\n",
    "for i, t in enumerate(timesteps):\n",
    "    sns.histplot(xt_distributions[i].numpy(), bins=50, stat='density', ax=axes[i], color='gray')\n",
    "    axes[i].set_title(f\"Timestep t = {t}\")\n",
    "    axes[i].set_xlim([-10, 10])\n",
    "    axes[i].set_xlabel(\"x_t\")\n",
    "    axes[i].set_ylabel(\"Density\")\n",
    "\n",
    "plt.suptitle(\"Forward Diffusion: How Clean Samples Become Noised Over Time\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------\n",
    "# Plot reverse diffusion trajectories (it is evident that we wind up with a multimodal mixed model distribution -- with more trajectories winding up at one mode (~2:1 ratio))\n",
    "# ---------------------------------------------\n",
    "def trace_multiple_trajectories(g, time_steps, beta, alpha, alpha_bar, num_samples=20):\n",
    "    \"\"\"\n",
    "    Tracks how multiple samples evolve from noise to data over time.\n",
    "    Returns a list of trajectories, one per sample.\n",
    "    \"\"\"\n",
    "    g.eval()\n",
    "    x = torch.randn(num_samples)  # Start from x_T ~ N(0, 1)\n",
    "    trajectories = [ [x[i].item()] for i in range(num_samples) ]\n",
    "\n",
    "    for t in reversed(range(1, time_steps)):\n",
    "        t_tensor = torch.full((num_samples,), t, dtype=torch.long)\n",
    "        z = torch.randn_like(x) if t > 1 else torch.zeros_like(x)\n",
    "        pred_epsilon = g(x, t_tensor)\n",
    "\n",
    "        x = (1 / torch.sqrt(alpha)) * (\n",
    "            x - (1 - alpha) / torch.sqrt(1 - alpha_bar[t]) * pred_epsilon\n",
    "        ) + torch.sqrt(beta) * z\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            trajectories[i].append(x[i].item())\n",
    "\n",
    "    return [traj[::-1] for traj in trajectories]  # Now traj[0] is x_0, traj[-1] is x_T\n",
    "\n",
    "# Get the trajectories\n",
    "trajectories = trace_multiple_trajectories(g, TIME_STEPS, BETA, ALPHA, ALPHABAR)\n",
    "\n",
    "# Plot them from right (x_T) to left (x_0)\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i, traj in enumerate(trajectories):\n",
    "    plt.plot(range(TIME_STEPS), traj, label=f\"Sample {i+1}\")\n",
    "plt.gca().invert_xaxis()  # Flip x-axis: right = noise, left = structure\n",
    "plt.title(\"Denoising Trajectories (Right = Noise, Left = Generated Sample)\")\n",
    "plt.xlabel(\"Timestep t (→ backward in time)\")\n",
    "plt.ylabel(\"x_t value\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"upper right\", fontsize=\"small\", ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "import ipykernel\n",
    "from notebook import notebookapp\n",
    "\n",
    "# Get current kernel connection file\n",
    "connection_file = ipykernel.get_connection_file()\n",
    "kernel_id = connection_file.split('-')[-1].split('.')[0]\n",
    "\n",
    "# Find all running servers\n",
    "for srv in notebookapp.list_running_servers():\n",
    "    try:\n",
    "        url = srv['url'] + 'api/sessions'\n",
    "        token = srv.get('token', '')\n",
    "        if token:\n",
    "            url += '?token=' + token\n",
    "        sessions = json.load(urllib.request.urlopen(url))\n",
    "        for sess in sessions:\n",
    "            if sess['kernel']['id'] == kernel_id:\n",
    "                notebook_path = sess['notebook']['path']\n",
    "                print(\"✅ Current notebook path:\", notebook_path)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
